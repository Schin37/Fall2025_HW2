package ragPipeline.Query

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import org.slf4j.LoggerFactory

import ragPipeline.models.OllamaJson.ChatMessage
import ragPipeline.models.{Ask, Ollama}
import ragPipeline.config.AppConfig

import scala.util.Try
import org.apache.hadoop.fs.{FileSystem, Path => HPath}

/**
 * Spark-based RAG query entry point that converts a published incremental snapshot into an
 * answer.  The driver performs three high-level phases:
 *   1. Resolve which retrieval snapshot (parquet table) to open, honoring CLI/System properties
 *      so the job can run against Hive tables, explicit paths, or the most recent incremental
 *      export directory.
 *   2. Embed the incoming question once on the driver, broadcast the vector, and let Spark score
 *      each stored chunk via cosine similarity so we only keep the global top-K passages.
 *   3. Stitch those passages into a readable context block and hand it to the chat model so the
 *      final answer is grounded in the retrieved evidence.
 *
 * Priority for locating the retrieval snapshot:
 *   1) -Drag.indexTable=rag.retrieval_index
 *   2) -Drag.indexPath=/abs/or/relative/path/to/out/retrieval_index
 *   3) AppConfig.paths.retrievalIndexDir (application.conf)
 *   4) Inferred from first arg (e.g., "out" -> "out/retrieval_index")
 *
 * Back-compat with old launcher:
 *   - If args has 1 item => it's the question.
 *   - If args has 2+ items => FIRST is shards dir (for inference), LAST is the question.
 */
object QueryMain {

  private val log = LoggerFactory.getLogger(getClass)
  private val DefaultTopK = 5

  def run(args: Array[String]): Unit = {
    if (args.isEmpty) {
      System.err.println(
        "Usage: QueryMain <question ...>\n" +
          "Back-compat: if two+ args are given, the FIRST is the shards dir (e.g., out) and the LAST is the question."
      )
      System.exit(1)
    }

    // If 2+ args: first is shardsParent, last is the question. If 1 arg: it's the question.
    val shardsParentOpt = if (args.length >= 2) Some(args.head.trim) else None
    val question        = args.last.trim

    val topK     = sys.props.get("rag.topK").flatMap(s => Try(s.toInt).toOption).getOrElse(DefaultTopK)
    val idxTable = sys.props.get("rag.indexTable")
    val idxPath  = sys.props.get("rag.indexPath")

    val embedModel = sys.props.getOrElse("rag.embedModel", AppConfig.models.embedModel)
    val chatModel  = sys.props.getOrElse("rag.chatModel",  AppConfig.models.chatModel)

    val spark = SparkSession.builder()
      .appName(s"RAG Query (Spark) topK=$topK")
      .master(sys.props.getOrElse("spark.master", "local[*]")) // default local mode
      .config("spark.ui.enabled", "false")
      .getOrCreate()
    import spark.implicits._

    // Helper: does path exist?
    def fsExists(p: String): Boolean = {
      if (p == null || p.isEmpty) false
      else {
        val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)
        try fs.exists(new HPath(p)) catch { case _: Throwable => false }
      }
    }
    // Walk a precedence ordered list of candidate paths and return the first that resolves.
    def firstExisting(paths: Seq[String]): Option[String] = paths.find(fsExists)

    // 3) Config default from AppConfig
    val cfgIndexOpt: Option[String] =
      Option(AppConfig.paths.retrievalIndexDir).map(_.toString).filter(fsExists)

    // 4) Infer from shards dir (Windows + POSIX variants)
    val inferredOpt: Option[String] = shardsParentOpt.flatMap { base =>
      firstExisting(Seq(
        s"$base/retrieval_index",
        s"$base\\retrieval_index",
        s"$base/out/retrieval_index",
        s"$base\\out\\retrieval_index"
      ))
    }

    // ---- 1) Load the published snapshot (priority order) ----
    // Decide which retrieval snapshot to read by walking the priority order documented above.
    val baseDF: DataFrame = (idxTable, idxPath, cfgIndexOpt, inferredOpt) match {
      case (Some(t), _, _, _) =>
        println(s"[Query] Reading table: $t")
        spark.table(t)

      case (None, Some(p), _, _) =>
        println(s"[Query] Reading parquet path (prop): $p")
        spark.read.parquet(p)

      case (None, None, Some(p), _) =>
        println(s"[Query] Reading parquet path (config): $p")
        spark.read.parquet(p)

      case (None, None, None, Some(p)) =>
        println(s"[Query] Reading inferred parquet path: $p")
        spark.read.parquet(p)

      case _ =>
        System.err.println(
          "ERROR: Could not find the retrieval snapshot.\n" +
            "Provide one of:\n" +
            "  -Drag.indexTable=<db.table>\n" +
            "  -Drag.indexPath=<parquet dir>\n" +
            "Or set paths.retrievalIndexDir in application.conf\n" +
            "Or pass a shards dir as the first arg (e.g., 'out') that contains 'retrieval_index/'."
        )
        spark.stop(); System.exit(2); return
    }

    // Narrow the snapshot to only the columns needed for scoring and attribution.
    val df = baseDF.select(
      $"docId", $"title", $"language",
      $"chunkId", $"chunkIx", $"sectionPath", $"chunkText",
      $"embedding" // Array[Float]
    )

    // ---- 2) Embed the question (Vector[String]) and broadcast ----
    val ollama = new Ollama()
    // The embedder returns a vector per input string.  We embed exactly one question here.
    val qEmbeds: Vector[Array[Float]] = ollama.embed(Vector(question), embedModel)
    val qVecF: Array[Float] = qEmbeds.headOption.getOrElse(Array.emptyFloatArray)
    if (qVecF.isEmpty) {
      System.err.println("ERROR: empty question embedding; check Ollama and embed model.")
      spark.stop(); System.exit(3); return
    }
    val qVec = qVecF.map(_.toDouble)
    val qB   = spark.sparkContext.broadcast(qVec)

    val toDouble = udf((xs: Seq[Float]) => if (xs == null) null else xs.map(_.toDouble))
    // Cosine similarity between the question (broadcast) and a stored embedding.  We keep the
    // implementation here instead of relying on MLlib so that this job stays dependency-free for
    // graders running in cluster-restricted environments.
    val cosSim = udf { (v: Seq[Double]) =>
      if (v == null) 0.0
      else {
        val q = qB.value
        val n = math.min(q.length, v.length)
        var i = 0; var dot = 0.0; var qn = 0.0; var vn = 0.0
        while (i < n) { val a = q(i); val b = v(i); dot += a*b; qn += a*a; vn += b*b; i += 1 }
        if (qn == 0.0 || vn == 0.0) 0.0 else dot / (math.sqrt(qn) * math.sqrt(vn))
      }
    }

    val scored =
      df.withColumn("embedding_d", toDouble(col("embedding")))
        .withColumn("score", cosSim(col("embedding_d")))
        .orderBy(col("score").desc)
        .limit(topK)
        .cache()

    // Pull the final top-K rows back to the driver to build the prompt fed to the chat model.
    val top = scored.select($"docId", $"title", $"sectionPath", $"chunkText", $"score")
      .as[(String, String, String, String, Double)]
      .collect()
      .toVector

    // ---- 3) Build context and query the chat model ----
    val context =
      top.zipWithIndex.map { case ((docId, title, section, text, s), i) =>
        val head = f"[$i] $title â€” $section (docId=$docId, score=$s%.4f)"
        s"$head\n$text"
      }.mkString("\n\n---\n\n")

    // The Ask helper wraps the question/context in an instruction that forces the LLM to cite the
    // retrieved text instead of hallucinating.
    log.info(s"LLM sizes: question=${question.length}, context=${context.length}")
    Console.println(s"Question=$question\n---\nContext Preview:\n$context")

    val messages: Vector[ChatMessage] = Ask.buildMessages(question, context)
    val answer = ollama.chat(messages, chatModel)

    Console.println("\n=== MODEL ANSWER ===\n" + answer + "\n=== MODEL ANSWER ===\n" )

    spark.stop()
  }
}

app {

  engine = "spark"     # or "mr"

  # --------- I/O (relative by default; pass s3a://... via CLI when on EMR) ---------
  pdfRoot   = "data/MSRCorpus"          # default local input (override with --in)
  outDir    = "out"                     # default local output (override with --out)
  retrievalIndexDir  = "out/retrieval_index"
  shardsDir = "out/index_shards"

  files {
    eachPartFilePrefix = "part-"
    names {
      docId   = "doc_id"
      chunkId = "chunk_id"
      text    = "text"
      vec     = "vec"
    }
  }

  models {
    embedModel = "nomic-embed-text"
    chatModel  = "llama3.2:3b"
    vectorDim  = 1024
    similarity = "cosine"               # cosine | dot | l2
  }

  embed {
    concurrency = 1       # start at 1; raise to 2 after it’s stable
    // 64–128
    batchSize   = 8   # you found 16 is fast
    maxChars    = 900     # per text cap (~300 tokens)
    minChunkChars = 300
  }

  models.ollama {
    baseUrl    = "http://127.0.0.1:11434"
    connectMs  = 30000   # 30s connect timeout
    readMs     = 180000  # 180s read timeout for big contexts
    model      = "llama3.2:3b"
    num_ctx    = 1024   # allow bigger context window
    temperature= 0.2
    top_p      = 0.95
  }

  query {
    perChunkCap = 600     # trim each chunk to this many characters
    maxCtxChars = 2000    # total context budget passed to the chat model
  }

  chunking {
    windowChars      = 900
    overlapPct       = 0.12
    maxContextBlock  = 2048
    NumShardBuckets = 32
  }

  mr {
    reducers        = 8
    reducersContext = 1                 # fixed typo (only this key exists now)
    topK            = 8
  }

  tests {
    neighborsTopK = 5

    wordSimilarity = [
      { a = "cat", b = "dog" }
      { a = "car", b = "truck" }
      { a = "bug", b = "error" }
      { a = "database", b = "storage" }
      { a = "software", b = "program" }
      { a = "quick",   b = "fast" }
      { a = "happy",   b = "joyful" }
      { a = "city",    b = "town" }
      { a = "river",   b = "stream" }
      { a = "ocean",   b = "sea" }
      { a = "teacher", b = "professor" }
      { a = "plane",   b = "aircraft" }
      { a = "email",   b = "message" }
      { a = "bug",     b = "defect" }
      { a = "query",   b = "question" }
    ]

    wordAnalogy = [
      { a = "king",   b = "man",     c = "woman",  expected = "queen" }
      { a = "paris",  b = "france",  c = "italy",  expected = "rome" }
      { a = "cpu",    b = "processor", c = "gpu",  expected = "accelerator" }
      { a = "boy",    b = "man",     c = "woman",  expected = "girl" }
      { a = "prince", b = "man",     c = "woman",  expected = "princess" }
      { a = "actor",  b = "man",     c = "woman",  expected = "actress" }
      { a = "cats",   b = "cat",     c = "dog",    expected = "dogs" }
      { a = "cars",   b = "car",     c = "plane",  expected = "planes" }
      { a = "rome",   b = "italy",   c = "france", expected = "paris" }
      { a = "madrid", b = "spain",   c = "italy",  expected = "rome" }
      { a = "ottawa", b = "canada",  c = "uk",     expected = "london" }
      { a = "beijing",b = "china",   c = "japan",  expected = "tokyo" }
      { a = "walking",b = "walk",    c = "swim",   expected = "swimming" }
    ]
  }

  aws {
    s3Bucket    = "s3://your-bucket/hw1"   # reference only; prefer passing --in/--out with s3a:// at runtime
    s3IndexPref = "lucene-index"
  }

  run {
    deleteOutputIfExists = true          # safe default; flip via CLI/env for dev
  }

  logging {
    level = "INFO"
  }
}

# Hadoop knobs you may want to centrally manage (empty by default; override per env)
hadoop.properties {
  # Example (uncomment as needed):
  # "fs.s3a.fast.upload" = "true"
  # "fs.s3a.aws.credentials.provider" = "com.amazonaws.auth.DefaultAWSCredentialsProviderChain"
  # "mapreduce.task.timeout" = "900000"
}
